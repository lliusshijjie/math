# 深度学习数学库开发路线图

## 当前已实现

| 模块 | 内容 |
|------|------|
| Core | 常量(PI, E)、clamp、lerp、特殊函数(gamma, beta, erf) |
| Linear Algebra | 静态Vector/Matrix、LU分解、线性方程组求解 |
| Calculus | 数值微分、数值积分、ODE求解器 |
| Probability | 连续/离散分布(Normal, Uniform, Gamma, Poisson等) |

## 待实现功能

### Phase 1: 激活函数与损失函数

```
math/nn/activations.hpp
├── sigmoid(x)
├── tanh(x)
├── relu(x)
├── leaky_relu(x, alpha)
├── elu(x, alpha)
├── gelu(x)
├── swish(x)
└── softmax(vec)

math/nn/loss.hpp
├── mse(pred, target)
├── cross_entropy(pred, target)
├── binary_cross_entropy(pred, target)
└── huber(pred, target, delta)
```

### Phase 2: 动态张量

```
math/tensor/tensor.hpp
├── Tensor<T> 动态维度
├── shape(), reshape(), flatten()
├── slice(), concat(), pad()
├── 逐元素运算: +, -, *, /, apply(func)
├── 广播机制
├── matmul(), bmm()
└── transpose(), permute()
```

### Phase 3: 自动微分

```
math/autograd/variable.hpp
├── Variable<T> 带梯度的张量
├── 计算图构建
├── backward() 反向传播
└── grad() 获取梯度

math/autograd/ops.hpp
├── 所有可微运算的前向/后向实现
└── 链式法则自动应用
```

### Phase 4: 优化器

```
math/optim/optimizer.hpp
├── SGD
├── Momentum
├── AdaGrad
├── RMSprop
└── Adam
```

### Phase 5: 初始化

```
math/nn/init.hpp
├── zeros(), ones()
├── uniform(low, high)
├── normal(mean, std)
├── xavier_uniform(), xavier_normal()
└── kaiming_uniform(), kaiming_normal()
```

### Phase 6: 卷积与池化 (可选)

```
math/nn/conv.hpp
├── conv2d()
├── max_pool2d()
├── avg_pool2d()
└── im2col(), col2im()
```

## 优先级

| 优先级 | Phase | 原因 |
|--------|-------|------|
| ★★★★★ | 1 | 简单，立即可用 |
| ★★★★★ | 2 | 深度学习基础设施 |
| ★★★★★ | 3 | 反向传播核心，工作量最大 |
| ★★★★☆ | 4 | 依赖Phase 3 |
| ★★★☆☆ | 5 | 辅助功能 |
| ★★☆☆☆ | 6 | CNN专用，可后期添加 |

## 目录结构规划

```
include/math/
├── core/
├── linalg/
├── calculus/
├── probability/
├── tensor/          [NEW]
│   └── tensor.hpp
├── autograd/        [NEW]
│   ├── variable.hpp
│   └── ops.hpp
├── nn/              [NEW]
│   ├── activations.hpp
│   ├── loss.hpp
│   ├── init.hpp
│   └── conv.hpp
└── optim/           [NEW]
    └── optimizer.hpp
```
